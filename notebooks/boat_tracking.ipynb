{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1/1: https://www.youtube.com/watch?v=K6xsEng2PhU... Success  (607 frames of shape 1920x1080 at 30.00 FPS)\n",
      "\n",
      "0: 384x640 1 car, 27.1ms\n",
      "0: 384x640 1 car, 25.3ms\n",
      "0: 384x640 1 car, 23.9ms\n",
      "0: 384x640 1 car, 23.7ms\n",
      "0: 384x640 1 car, 23.9ms\n",
      "0: 384x640 1 car, 23.4ms\n",
      "0: 384x640 1 car, 23.6ms\n",
      "0: 384x640 1 car, 24.1ms\n",
      "0: 384x640 1 car, 24.3ms\n",
      "0: 384x640 1 car, 24.4ms\n",
      "0: 384x640 1 car, 21.9ms\n",
      "0: 384x640 1 car, 22.4ms\n",
      "0: 384x640 1 car, 21.8ms\n",
      "0: 384x640 1 car, 22.3ms\n",
      "0: 384x640 1 car, 21.6ms\n",
      "0: 384x640 1 car, 23.8ms\n",
      "0: 384x640 1 car, 21.4ms\n",
      "0: 384x640 1 car, 22.2ms\n",
      "0: 384x640 1 car, 22.0ms\n",
      "0: 384x640 1 car, 22.7ms\n",
      "0: 384x640 1 car, 22.4ms\n",
      "0: 384x640 1 car, 21.3ms\n",
      "0: 384x640 1 car, 22.1ms\n",
      "0: 384x640 1 car, 21.7ms\n",
      "0: 384x640 1 car, 21.2ms\n",
      "0: 384x640 1 car, 22.1ms\n",
      "0: 384x640 1 car, 22.5ms\n",
      "0: 384x640 1 car, 21.5ms\n",
      "0: 384x640 1 car, 21.3ms\n",
      "0: 384x640 1 car, 21.4ms\n",
      "0: 384x640 1 car, 21.4ms\n",
      "0: 384x640 1 car, 21.7ms\n",
      "0: 384x640 1 car, 21.3ms\n",
      "0: 384x640 1 car, 21.0ms\n",
      "0: 384x640 1 car, 21.4ms\n",
      "0: 384x640 1 car, 21.2ms\n",
      "0: 384x640 1 car, 20.9ms\n",
      "0: 384x640 1 car, 21.6ms\n",
      "0: 384x640 1 car, 21.3ms\n",
      "0: 384x640 1 car, 21.6ms\n",
      "0: 384x640 1 car, 21.2ms\n",
      "0: 384x640 1 car, 20.3ms\n",
      "0: 384x640 1 car, 20.8ms\n",
      "0: 384x640 1 car, 20.5ms\n",
      "0: 384x640 1 car, 20.4ms\n",
      "0: 384x640 1 car, 19.2ms\n",
      "0: 384x640 1 car, 19.4ms\n",
      "0: 384x640 1 car, 19.6ms\n",
      "0: 384x640 1 car, 19.8ms\n",
      "0: 384x640 1 car, 20.9ms\n",
      "0: 384x640 1 car, 19.6ms\n",
      "0: 384x640 1 car, 19.5ms\n",
      "0: 384x640 1 car, 20.2ms\n",
      "0: 384x640 1 car, 19.7ms\n",
      "0: 384x640 1 car, 18.8ms\n",
      "0: 384x640 (no detections), 19.5ms\n",
      "0: 384x640 (no detections), 19.4ms\n",
      "0: 384x640 (no detections), 19.0ms\n",
      "0: 384x640 (no detections), 19.6ms\n",
      "0: 384x640 (no detections), 19.4ms\n",
      "0: 384x640 (no detections), 18.9ms\n",
      "0: 384x640 (no detections), 19.0ms\n",
      "0: 384x640 (no detections), 17.4ms\n",
      "0: 384x640 (no detections), 17.5ms\n",
      "0: 384x640 (no detections), 17.8ms\n",
      "0: 384x640 (no detections), 19.6ms\n",
      "0: 384x640 (no detections), 18.7ms\n",
      "0: 384x640 (no detections), 18.2ms\n",
      "0: 384x640 (no detections), 19.7ms\n",
      "0: 384x640 (no detections), 20.5ms\n",
      "0: 384x640 (no detections), 19.2ms\n",
      "0: 384x640 3 cars, 18.8ms\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 59\u001b[0m\n\u001b[0;32m     57\u001b[0m track_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result\u001b[38;5;241m.\u001b[39mboxes, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m---> 59\u001b[0m     track_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mboxes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     60\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mputText(annotated_frame, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mID:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrack_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, (center_x \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m10\u001b[39m, center_y \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m10\u001b[39m),\n\u001b[0;32m     61\u001b[0m                 cv2\u001b[38;5;241m.\u001b[39mFONT_HERSHEY_SIMPLEX, \u001b[38;5;241m0.5\u001b[39m, (\u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# Check if the car's center has crossed the vertical line\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# and if it hasn't been captured already.\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "output_dir = \"../outputs/tracked_vehicles\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load YOLO models (assuming class 2 corresponds to cars)\n",
    "detection_model = YOLO(\"../yolo_weights/yolo12x.pt\")\n",
    "segmentation_model = YOLO(\"../yolo_weights/yolo11x-seg.pt\")\n",
    "\n",
    "# Define a vertical line at x = 400 pixels from the left\n",
    "line_x = 400\n",
    "\n",
    "# Run inference in stream mode\n",
    "results = detection_model.track(\n",
    "    source=\"https://www.youtube.com/watch?v=K6xsEng2PhU\", \n",
    "    stream_buffer=True,\n",
    "    stream=True, \n",
    "    persist=True, \n",
    "    vid_stride=1, \n",
    "    classes=[2]\n",
    ")\n",
    "\n",
    "# Set to store IDs of cars already captured\n",
    "captured_ids = set()\n",
    "frame_id = 0  # Counter for saved frames\n",
    "\n",
    "# Process each frame from the stream\n",
    "for result in results:\n",
    "    # Get the annotated frame with built-in detections\n",
    "    annotated_frame = result.plot()  # image with bounding boxes & labels\n",
    "    frame_height, frame_width = annotated_frame.shape[:2]\n",
    "\n",
    "    # Draw the vertical line for reference\n",
    "    cv2.line(annotated_frame, (line_x, 0), (line_x, frame_height), (0, 255, 0), 2)\n",
    "\n",
    "    save_frame = False  # Flag to decide if this frame should be saved\n",
    "\n",
    "    # Process each detected box (assumed to be in result.boxes.xyxy)\n",
    "    if result.boxes is not None and len(result.boxes.xyxy) > 0:\n",
    "        for i, box in enumerate(result.boxes.xyxy):\n",
    "            # Get bounding box coordinates and compute the center\n",
    "            if hasattr(box, \"cpu\"):\n",
    "                coords = [int(coord) for coord in box.cpu().numpy()]\n",
    "            else:\n",
    "                coords = [int(coord) for coord in box]\n",
    "            x1, y1, x2, y2 = coords\n",
    "            center_x = int((x1 + x2) / 2)\n",
    "            center_y = int((y1 + y2) / 2)\n",
    "\n",
    "            # Annotate the center with a small blue circle\n",
    "            cv2.circle(annotated_frame, (center_x, center_y), 5, (255, 0, 0), -1)\n",
    "\n",
    "            # Optionally annotate with tracking ID (if available)\n",
    "            track_id = None\n",
    "            if hasattr(result.boxes, \"id\"):\n",
    "                track_id = int(result.boxes.id[i])\n",
    "                cv2.putText(annotated_frame, f\"ID:{track_id}\", (center_x - 10, center_y - 10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "\n",
    "            # Check if the car's center has crossed the vertical line\n",
    "            # and if it hasn't been captured already.\n",
    "            if center_x >= line_x and track_id is not None and track_id not in captured_ids:\n",
    "                save_frame = True\n",
    "                captured_ids.add(track_id)\n",
    "\n",
    "    # Save the frame if a new car has crossed the line\n",
    "    if save_frame:\n",
    "        filename = os.path.join(output_dir, f\"frame_{frame_id:05d}.jpg\")\n",
    "        cv2.imwrite(filename, annotated_frame)\n",
    "        frame_id += 1\n",
    "\n",
    "    # Display the frame in a window\n",
    "    cv2.imshow(\"Vehicle Detection\", annotated_frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'isnumeric'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m results \u001b[38;5;241m=\u001b[39m detection_model\u001b[38;5;241m.\u001b[39mtrack(source\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.youtube.com/watch?v=e_WBuBqS9h8\u001b[39m\u001b[38;5;124m\"\u001b[39m, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, classes\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m2\u001b[39m])\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Process each frame from the stream\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresults\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Retrieve the original frame (BGR format)\u001b[39;49;00m\n\u001b[0;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morig_img\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe_height\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_width\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32md:\\files_main\\projects_ml\\fish-height-estimation\\fish-height-estimation\\venv_fhe\\Lib\\site-packages\\torch\\utils\\_contextlib.py:36\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m---> 36\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     40\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[1;32md:\\files_main\\projects_ml\\fish-height-estimation\\fish-height-estimation\\venv_fhe\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:233\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[1;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msetup_model(model)\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:  \u001b[38;5;66;03m# for thread-safe inference\u001b[39;00m\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;66;03m# Setup source every time predict is called\u001b[39;00m\n\u001b[1;32m--> 233\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup_source\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msource\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;66;03m# Check if save_dir/ label file exists\u001b[39;00m\n\u001b[0;32m    236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave_txt:\n",
      "File \u001b[1;32md:\\files_main\\projects_ml\\fish-height-estimation\\fish-height-estimation\\venv_fhe\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:205\u001b[0m, in \u001b[0;36mBasePredictor.setup_source\u001b[1;34m(self, source)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimgsz \u001b[38;5;241m=\u001b[39m check_imgsz(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mimgsz, stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mstride, min_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# check image size\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[0;32m    198\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    204\u001b[0m )\n\u001b[1;32m--> 205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_inference_source\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[43m    \u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvid_stride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvid_stride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39msource_type\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_type\u001b[38;5;241m.\u001b[39mstream\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_type\u001b[38;5;241m.\u001b[39mscreenshot\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1000\u001b[39m  \u001b[38;5;66;03m# many images\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvideo_flag\u001b[39m\u001b[38;5;124m\"\u001b[39m, [\u001b[38;5;28;01mFalse\u001b[39;00m]))\n\u001b[0;32m    217\u001b[0m ):  \u001b[38;5;66;03m# videos\u001b[39;00m\n",
      "File \u001b[1;32md:\\files_main\\projects_ml\\fish-height-estimation\\fish-height-estimation\\venv_fhe\\Lib\\site-packages\\ultralytics\\data\\build.py:208\u001b[0m, in \u001b[0;36mload_inference_source\u001b[1;34m(source, batch, vid_stride, buffer)\u001b[0m\n\u001b[0;32m    206\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m source\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m stream:\n\u001b[1;32m--> 208\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mLoadStreams\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvid_stride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvid_stride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m screenshot:\n\u001b[0;32m    210\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m LoadScreenshots(source)\n",
      "File \u001b[1;32md:\\files_main\\projects_ml\\fish-height-estimation\\fish-height-estimation\\venv_fhe\\Lib\\site-packages\\ultralytics\\data\\loaders.py:116\u001b[0m, in \u001b[0;36mLoadStreams.__init__\u001b[1;34m(self, sources, vid_stride, buffer)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m urlparse(s)\u001b[38;5;241m.\u001b[39mhostname \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwww.youtube.com\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myoutube.com\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myoutu.be\u001b[39m\u001b[38;5;124m\"\u001b[39m}:  \u001b[38;5;66;03m# if source is YouTube video\u001b[39;00m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;66;03m# YouTube format i.e. 'https://www.youtube.com/watch?v=Jsn8D3aC840' or 'https://youtu.be/Jsn8D3aC840'\u001b[39;00m\n\u001b[0;32m    115\u001b[0m     s \u001b[38;5;241m=\u001b[39m get_best_youtube_url(s)\n\u001b[1;32m--> 116\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28meval\u001b[39m(s) \u001b[38;5;28;01mif\u001b[39;00m \u001b[43ms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misnumeric\u001b[49m() \u001b[38;5;28;01melse\u001b[39;00m s  \u001b[38;5;66;03m# i.e. s = '0' local webcam\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m s \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m (IS_COLAB \u001b[38;5;129;01mor\u001b[39;00m IS_KAGGLE):\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    119\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource=0\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m webcam not supported in Colab and Kaggle notebooks. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    120\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTry running \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource=0\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m in a local environment.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    121\u001b[0m     )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'isnumeric'"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 4 boats, 195.1ms\n",
      "Speed: 3.0ms preprocess, 195.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 boats, 200.2ms\n",
      "Speed: 2.0ms preprocess, 200.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 boats, 210.1ms\n",
      "Speed: 2.0ms preprocess, 210.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 boats, 211.0ms\n",
      "Speed: 5.0ms preprocess, 211.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 boats, 205.0ms\n",
      "Speed: 1.1ms preprocess, 205.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 boats, 187.5ms\n",
      "Speed: 2.4ms preprocess, 187.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 boats, 194.4ms\n",
      "Speed: 6.5ms preprocess, 194.4ms inference, 8.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 boats, 207.0ms\n",
      "Speed: 0.0ms preprocess, 207.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 boats, 187.4ms\n",
      "Speed: 9.3ms preprocess, 187.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 boats, 189.6ms\n",
      "Speed: 0.0ms preprocess, 189.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 boats, 183.1ms\n",
      "Speed: 8.0ms preprocess, 183.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 boats, 195.1ms\n",
      "Speed: 0.0ms preprocess, 195.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 boats, 216.2ms\n",
      "Speed: 0.0ms preprocess, 216.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 boats, 199.1ms\n",
      "Speed: 9.9ms preprocess, 199.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 boats, 181.7ms\n",
      "Speed: 0.0ms preprocess, 181.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 boats, 199.2ms\n",
      "Speed: 0.0ms preprocess, 199.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 boats, 212.2ms\n",
      "Speed: 5.3ms preprocess, 212.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 boats, 193.2ms\n",
      "Speed: 2.7ms preprocess, 193.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 boats, 197.9ms\n",
      "Speed: 2.0ms preprocess, 197.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 boats, 201.4ms\n",
      "Speed: 8.1ms preprocess, 201.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 boats, 216.0ms\n",
      "Speed: 0.0ms preprocess, 216.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 boats, 204.8ms\n",
      "Speed: 1.9ms preprocess, 204.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 boats, 200.9ms\n",
      "Speed: 0.0ms preprocess, 200.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 boats, 209.1ms\n",
      "Speed: 0.0ms preprocess, 209.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 boats, 208.5ms\n",
      "Speed: 0.0ms preprocess, 208.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 boats, 210.7ms\n",
      "Speed: 0.0ms preprocess, 210.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 boats, 219.6ms\n",
      "Speed: 0.0ms preprocess, 219.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 boats, 212.8ms\n",
      "Speed: 0.0ms preprocess, 212.8ms inference, 8.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 boats, 215.8ms\n",
      "Speed: 0.0ms preprocess, 215.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 boats, 220.3ms\n",
      "Speed: 2.0ms preprocess, 220.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 boats, 208.5ms\n",
      "Speed: 3.0ms preprocess, 208.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 boats, 224.5ms\n",
      "Speed: 0.0ms preprocess, 224.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 boats, 207.9ms\n",
      "Speed: 5.7ms preprocess, 207.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 boats, 208.3ms\n",
      "Speed: 8.0ms preprocess, 208.3ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 boats, 207.9ms\n",
      "Speed: 0.0ms preprocess, 207.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 boats, 209.4ms\n",
      "Speed: 0.0ms preprocess, 209.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 boats, 206.6ms\n",
      "Speed: 2.6ms preprocess, 206.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 boats, 219.6ms\n",
      "Speed: 0.0ms preprocess, 219.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 boats, 206.7ms\n",
      "Speed: 0.0ms preprocess, 206.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 boats, 202.7ms\n",
      "Speed: 2.3ms preprocess, 202.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 boats, 207.4ms\n",
      "Speed: 8.2ms preprocess, 207.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 boats, 217.4ms\n",
      "Speed: 0.0ms preprocess, 217.4ms inference, 8.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 boats, 223.2ms\n",
      "Speed: 8.0ms preprocess, 223.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture()\n",
    "# Define the x-coordinate of the black line (on the orange post)\n",
    "black_line_x = 1090  # Adjust based on your specific video\n",
    "# Define the x-coordinate of the blue line (400 pixels to the right)\n",
    "blue_line_x = black_line_x + 400\n",
    "# Tolerance (pixelwise)\n",
    "tolerance = 5\n",
    "\n",
    "# Loop through the video frames\n",
    "while cap.isOpened():\n",
    "    # Read a frame from the video\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        break  # Break the loop if the end of the video is reached\n",
    "\n",
    "    # Run YOLO inference on the frame, filtering for the 'boat' class (class index 8)\n",
    "    results = detection_model.track(frame, classes=[8], persist=True)\n",
    "    # Visualize the results on the frame\n",
    "    annotated_frame = results[0].plot()\n",
    "\n",
    "    # Iterate over detected objects\n",
    "    for result in results:\n",
    "        boxes = result.boxes\n",
    "        for box in boxes:\n",
    "            # Extract class ID and confidence\n",
    "            class_id = int(box.cls)\n",
    "            confidence = box.conf\n",
    "\n",
    "            # Filter for boats with a confidence threshold\n",
    "            if class_id == 8 and confidence > 0.2:\n",
    "                # Extract bounding box coordinates\n",
    "                xyxy = box.xyxy.cpu().numpy().flatten()\n",
    "                x1, y1, x2, y2 = map(int, xyxy)\n",
    "\n",
    "                # Calculate center of mass (centroid)\n",
    "                center_x = (x1 + x2) // 2\n",
    "                center_y = (y1 + y2) // 2\n",
    "\n",
    "                # Draw center of mass\n",
    "                cv2.circle(annotated_frame, (center_x, center_y), 5, (0, 255, 255), -1)\n",
    "\n",
    "                # Check if the boat's center crosses the blue line\n",
    "                if blue_line_x - tolerance <= center_x <= blue_line_x + tolerance:\n",
    "                    # Use vessel and format ID properly in filename\n",
    "                    vessel_id = int(box.id.cpu().numpy()) if box.id is not None else 0\n",
    "                    filename = os.path.join(output_dir, f\"vessel_id{vessel_id}_detected.png\")\n",
    "                    cv2.imwrite(filename, frame, [cv2.IMWRITE_PNG_COMPRESSION, 0])  # Save with no compression for highest quality\n",
    "                    print(f\"Frame saved: {filename}\")\n",
    "              \n",
    "    # Draw the black vertical line on the orange post\n",
    "    cv2.line(annotated_frame, (black_line_x, 0), (black_line_x, frame.shape[0]), (0, 0, 0), 10)\n",
    "    # Draw the bright blue vertical line 400 pixels to the right\n",
    "    cv2.line(annotated_frame, (blue_line_x, 0), (blue_line_x, frame.shape[0]), (255, 0, 0), 10)        \n",
    "    # Display the annotated frame\n",
    "    cv2.imshow(\"YOLO Inference\", annotated_frame)\n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close the display window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_vhe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
